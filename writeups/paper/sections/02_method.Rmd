
```{r read in data}
DATA_PATH <- here("data/processed/syntactic_bootstrapping_tidy_data.csv") 
RAW_DATA_PATH <- here("data/raw/syntactic_bootstrapping_raw_data.csv")

ma_data <- read_csv(DATA_PATH)
```


## Literature Search  

```{r literature search - search procedure}
records_identified_google_scholar <- 2330
records_identified_forward_search <- 1009
total_search_records_identified <- prettyNum(records_identified_google_scholar + records_identified_forward_search,
                                      big.mark=",",scientific=FALSE)

records_identified_review_reference <- 155 
records_identified_experts_in_the_field <- 11


records_screened_google_scholar <- 10 * 60 
records_screened_forward_search <- 10 * 10 
records_screened_review_reference <- 155 
records_screened_experts_in_the_field <- 14

total_records_screened <- records_screened_google_scholar +
  records_screened_forward_search + 
  records_screened_review_reference + 
  records_screened_experts_in_the_field

# class project + else
full_text_assesed_for_eligibility <- 503 + 166
final_inclusion <- read_csv(RAW_DATA_PATH) %>% 
  filter(paper_eligibility == "include") %>%
  distinct(unique_id) %>%
  count() - 1 # minus one because we decided to count the two conditions from Naigles's unpublished study (visit5, visit 6 as one paper)

screen_exclusions <- total_records_screened - full_text_assesed_for_eligibility
full_text_assesed_exclusion <- full_text_assesed_for_eligibility -
  final_inclusion
```


We conducted a literature search of the syntactic bootstrapping literature following the  Preferred Reporting Items for Systematic Reviews and Meta-Analyses checklist [PRISMA; @moher2009]. We identified relevant papers  through a keyword search in Google Scholar with the phrase "syntactic bootstrapping" and a forward search on papers that cited the seminal paper, @naigles1990children (total records identified: *N* =  `r total_search_records_identified`; retrieved between May 2020 and July 2020; Figure\ \@ref(fig:prisma)). We screened the abstracts  for relevance of the first $60$ pages of the keyword search results (*N* = $600$) and the first $10$ pages of the forward search results (*N* = $100$). The screening processes ended because we could no longer identify relevant, non-duplicate papers. Additional papers were identified by consulting the references section of a recent literature review [*N* = $`r records_identified_review_reference`$; @fisher2020developmental] and experts in the field (*N* = $`r records_identified_experts_in_the_field`$). Our sample included published journal articles, conference proceedings, doctoral dissertations, and unpublished manuscripts. We refer to these collectively as "papers" in the following sections. 

```{r literature}
n_papers <- ma_data %>%
  distinct(unique_id) %>%
  nrow() - 1 # minus one because we decided to count the two conditions from Naigles's unpublished study (visit5, visit 6 as one paper)
```
  
  
We restricted our final sample to papers that satisfied the following criteria: First, the experimental paradigm involved a two-alternative forced-choice task in which  participants were instructed to identify the scene that matched the linguistic stimuli. Second, the visual stimuli were two events displayed side-by-side on a computer monitor. One event depicted a causative action (e.g., one agent causes the other to move), and the other a non-causative action (e.g., two agents move simultaneously but do not causally interact with each other). We included studies with either videos of live actors or animated clips. Third, the linguistic stimuli included at least one novel verb embedded in a syntactically informative frame. For example, "Look, it's kradding!" embeds the novel verb in an intransitive syntactic frame that is informative about the meaning of the novel verb "kradding". In contrast, "Look, kradding!" does not provide informative syntactic information.  Finally, we restricted our sample to studies with English-speaking, typically-developing children. Papers that satisfied these constraints reflected a range of methodological implementations that we examine systematically below (see Moderators section). Our final sample included  `r n_papers` papers, indicated by an asterisks in the reference section.

```{r prisma, fig.cap = "PRISMA plot showing literature review process. Values indicate number of papers at each stage of the review review process. Common exclusion reasons include i) the manuscripts did not include empirical studies; ii) the manuscripts were written in languages other than English; iii) the empirical studies did not satisfy inclusion criteria the entry. Our meta-analysis included a final sample of 17 papers. "}
prisma2(found = format(records_identified_google_scholar + records_identified_forward_search, big.mark = ","), 
        found_other = records_identified_review_reference + records_identified_experts_in_the_field,
        no_dupes = total_records_screened, 
        screened = total_records_screened, 
        screen_exclusions = total_records_screened - full_text_assesed_for_eligibility, 
        full_text = full_text_assesed_for_eligibility,
        full_text_exclusions = full_text_assesed_exclusion, 
        quantitative = final_inclusion,
        font_size = 10,
        dpi = 50
      )
```



   
## Data Entry

```{r}
n_effect_sizes <- ma_data %>%
  filter(!is.na(d_calc)) %>%
  nrow()
```

For each paper, we entered metadata about the paper (e.g., citation), information to calculate effect sizes, and information about moderators. We entered a separate effect size for each experimental manipulation and age group per paper (we refer to these as "conditions"). Most papers therefore contained multiple conditions, corresponding to multiple effect sizes in our meta-analysis. Our final sample included `r n_effect_sizes` conditions (*N*). 

```{r data_source_summary}
data_source_summary <- ma_data %>% 
  group_by(data_source_clean) %>% 
  count()

num_author_contact <- data_source_summary %>% 
  filter(data_source_clean == "author_contact") %>% 
  pull(n)

num_plot <- data_source_summary %>% 
  filter(data_source_clean == "plot") %>% 
  pull(n)

num_text_or_table <- data_source_summary %>% 
  filter(data_source_clean == "text/table") %>% 
  pull(n)

num_imputed <- data_source_summary %>% 
  filter(data_source_clean == "imputed") %>% 
  pull(n)
```

### Calculating individual effect sizes
For each condition, we recorded the sample size, the  mean proportion correct responses, and the across-participant standard deviation of proportion correct responses. The mean and standard deviation were obtained from one of  four sources: (i) text or tables in the results section (*N* = `r num_text_or_table`); (ii) plots (*N* =`r num_plot`); (iii) correspondence with the original authors (*N* = `r num_author_contact`); and (iv) imputation using values from studies with similar designs (*N* = `r num_imputed`, @hirsh1996young; the missing standard deviation values were imputed from @naigles1990children). Previous work suggests using imputed values from highly similar studies improves the accuracy of effect size estimates [@furukawa2006imputing]. The reported results do not qualitatively change when conditions from @hirsh1996young are excluded from our sample (see SI, Sec. 4)^[Supplemental Information available at https://rpubs.com/anjiecao/SBMA_SI.].

```{r data_preprocessing}
num_has_raw <- ma_data %>% 
  filter(!is.na(x_2_raw)) %>% 
  distinct(unique_id) %>%  
  nrow()
```

Using the raw coded data, we calculated an effect size estimate for each condition as Cohen's *d*.  Cohen's *d* was calculated as the difference between the proportion correct responses and chance (.5), divided by a pooled estimate of variance (see SI, Sec. 1 for example calculation). Note that we assume baseline performance to be .5 in all cases, even when a neutral empirical baseline was reported (e.g., Arunachalam & Dennis, 2019; *N* = `r num_has_raw` conditions). This decision aligns with the often-used assumption in probability theory and modeling work that choices from a finite set are independent from each other [@luce1959individual; @frank2012predicting]. This decision affords the practical advantage that effect sizes can be estimated across all conditions in our sample, most of which did not report an empirical baseline, and that the meta-analytic effect size estimate can be directly compared to estimates for other word learning phenomena that also use .5 as a baseline in similiar two-alternative fourced-choice paradigms [e.g., @lewis2020role;@fort2018symbouki]. An alternative method of estimating effect sizes is to use performance in the intransitive condition as a baseline, and calculate effect sizes only for transitive sentences. This approach controls for baseline differences in perceptual stimuli (assuming that saliency effects are additive), but does not allow for estimates to be calculated consistently across all studies in our sample, and does not allow for effect sizes to be estimated for intransitive conditions (see SI, Sec. 2 for direct comparison between two methods). Our approach provides a theory-neutral, consistent method for calculating effect sizes across the syntactic bootstrapping literature. 

### Moderators

```{r categorize_by_proto}
num_proto_approach <- ma_data %>% 
  filter (inclusion_certainty == 2) %>% 
  count()

num_atypical_approach <- ma_data %>% 
  filter (inclusion_certainty == 1) %>%
  count()
```


```{r vocb_available}
num_vocabulary_available <- ma_data %>%
  filter(!is.na(productive_vocab_median)) %>% 
  nrow()
```


```{r categorize_by_linguistic_stimuli}
num_transitive <- ma_data %>% filter(sentence_structure == "transitive") %>% nrow()
num_intransitive <- ma_data %>% filter(sentence_structure == "intransitive") %>% nrow()

num_agent_sum <- ma_data %>% count(agent_argument_type)
num_agent_noun <- num_agent_sum %>% filter(agent_argument_type == "noun") %>% pull(n)
num_agent_pronoun <- num_agent_sum %>% filter(agent_argument_type == "pronoun") %>% pull(n)

num_patient_sum <-  ma_data %>% count(patient_argument_type)
num_patient_noun <- num_patient_sum %>% filter(patient_argument_type == "noun") %>% pull(n)
num_patient_pronoun <- num_patient_sum %>% filter(patient_argument_type == "pronoun") %>% pull(n)
num_patient_intransitive <- num_patient_sum %>% filter(patient_argument_type == "intransitive") %>% pull(n)
```


For each effect size in our sample, we coded several theoretical and methodological variables. The information was  retrieved either from the methods section of the paper or by contacting authors. 

Four theoretical variables were coded: participant age, participant vocabulary size, predicate type, and noun phrase type. Participant age was entered in mean age in months (*N* = `r n_effect_sizes` conditions). Vocabulary size was recorded as the median productive vocabulary measured by MacArthur-Bates Communicative Development Inventories (CDI) Words and Sentences [@fenson2000short; *N* = `r  num_vocabulary_available`].  Predicate type was coded as either transitive (*N* = `r num_transitive`) or intransitive (*N* = `r num_intransitive`). Noun phrase type encoded information about the agent verb argument of the sentence stimulus. The agent of the sentence was coded as being either a noun (e.g., "the girl"; *N* = `r num_agent_noun`) or pronoun ("she"; *N* = `r num_agent_pronoun`). A condition was coded as "pronoun" if it contained at least one instance of a pronoun that referred to the agent. 


```{r categorize_by_testing_phase}
num_practice_yes <- ma_data %>% filter(practice_phase == "yes") %>% nrow()
num_practice_no <- ma_data %>% filter(practice_phase == "no") %>% nrow()
num_char_id_yes <- ma_data %>% filter(character_identification == "yes") %>% nrow()
num_char_id_no <- ma_data %>% filter(character_identification == "no") %>% nrow()
```

```{r categorize_by_synchronicity}
num_video_simultaneous <- ma_data %>% filter(presentation_type == "simultaneous") %>% nrow()
num_video_asynchronous <- ma_data %>% filter(presentation_type == "asynchronous") %>% nrow()
```

```{r categorize_by_testing_structure}
num_mass <- ma_data %>% filter(test_mass_or_distributed == "mass") %>% nrow()
num_distributed <- ma_data %>% filter(test_mass_or_distributed == "distributed") %>% nrow()
```


In addition to the theoretical variables, we coded a range of methodological variables that varied across the studies in our sample and for which there was independent reason to predict that they could influence the size of the effect. First, we coded whether the paradigm included a practice trial prior to the testing phase. A study was coded as having a practice trial if there was at least one trial in which children were presented with a familiar verb and asked to identify a familiar action (e.g., "Find jumping"; *N* conditions with practice phase =  `r num_practice_yes`). Second, we coded whether or not the paradigm involved trials in which  children were prompted to identify the nouns in the testing events (e.g., "Where's the bunny?"; *N* with character identification phase = `r num_char_id_yes`). Third, we coded whether the linguistic and visual stimuli were presented synchronously with each other ("Stimuli Synchronicity"). An experimental condition was coded as  "asynchronous" if the linguistic stimulus was first paired with an irrelevant visual scene (e.g. a person on the phone talking), and the matching visual stimulus was not shown until the training phase was over (*N* = `r num_video_asynchronous`); a condition was coded as "simultaneous" if the very first training sentence was presented along with the visual stimuli depicting the relevant action or along with an attention-getter or a blank screen, immediately followed by the relevant action  (*N* = `r num_video_simultaneous`). Fourth, we coded the temporal distribution of the training and the testing trials (Mass: *N* = `r num_mass`; Distributed: *N* = `r num_distributed`). The temporal distribution is linked to the amount of learning experience children have prior to the test. A procedure was categorized as "mass" if participants were trained exclusively on one novel verb and tested on the same verb, and "distributed" if they were trained and tested on multiple novel verbs. Finally, we coded how many times each novel verb was spoken in a syntactically-informative way during training. ^[See SI, Sec. 6 for additional methodological moderators. These additional moderators overlap substantially with the target moderators of interest presented in the Main Text, but are included in the SI for completeness.]


## Analytic Approach 

We analyzed the data using multi-level random effect models implemented in the metafor package in R [@viechtbauer2010]. The random effect structure included groupings by paper and by participant group (i.e. cases where the same participants were tested across multiple conditions) to account for the clustering of effect sizes in our sample. The sensitivity analysis was conducted using the PublicationBias package in R [@mathur2020sensitivity]. Moderator variables were included as additive fixed effects. All estimate ranges correspond to 95% confidence intervals unless otherwise noted.  Data and analysis scripts are available in the project repository (https://github.com/anjiecao/SyntacticBootstrappingMA), and the dataset can be interactively explored on Metalab (http://metalab.stanford.edu/).
