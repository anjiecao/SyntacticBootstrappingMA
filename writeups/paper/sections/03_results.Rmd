


```{r generate to-print model results}



# MODERATORS <- c( "NULL", "mean_age_months","productive_vocab_median", "sentence_structure", "agent_argument_type", "patient_argument_type", "n_repetitions_sentence", "n_repetitions_video", "stimuli_modality", "stimuli_actor", "transitive_event_type","intransitive_event_type", "visual_stimuli_pair", "test_method","presentation_type","character_identification", "practice_phase", "test_mass_or_distributed", "n_train_test_pair", "n_test_trial_per_pair" )

MODERATORS <- c( "NULL", "mean_age_months","productive_vocab_median", "sentence_structure", "agent_argument_type", "patient_argument_type", "n_repetitions_sentence", "stimuli_modality", "stimuli_actor",  "presentation_type","character_identification", "practice_phase", "test_mass_or_distributed", "n_train_test_pair", "n_test_trial_per_pair" )

mod_print <- generate_moderator_df(MODERATORS, ma_data)
```

```{r label = "forest", fig.pos = "!H", fig.cap = "Forest plot showing individual effect sizes included in the meta-analysis. Black circles and triangles correspond to individual conditions with intransitive sentences and transitive sentences, respectively. Point size corresponds to sample size, and  horizontal error bars show  95% confidence intervals (note that the confidence interval for one estimate, Yuan, Fisher, & Snedeker, 2012, 3a, is elided for readability).  Each effect size is labeled with the author and year of the source paper and an experiment number and condition identifer. Negative effect sizes indicate that children looked longer at the incorrect action. The red diamond indicates the meta-analytic effect size aggregated across all conditions in the literature.", fig.width = 5, fig.height = 7.5}
generate_forest_plot(ma_data)
```

```{r}

unique_infants <- ma_data %>% 
  summarize(sum_infants = sum(n_1))  %>%
  pull(sum_infants)

mean_age_day <- ma_data %>% 
  filter(!is.na(mean_age)) %>%
  summarize(mean_age =  format(round(mean(mean_age), 2))) %>%
  pull(mean_age) 

SD_age_day <- ma_data %>% 
  filter(!is.na(mean_age)) %>%
  summarize(sd_age =  format(round(sd(mean_age), 2))) %>%
  pull(sd_age) 

min_age_day <- ma_data %>% 
  filter(!is.na(mean_age)) %>%
  summarize(min_age_day =  (round(min(mean_age), 2))) %>% 
  pull(min_age_day)

max_age_day <- ma_data %>% 
  filter(!is.na(mean_age)) %>%
  summarize(max_age_day =  (round(max(mean_age), 2))) %>% 
  pull(max_age_day)

mean_age_months <- floor(as.numeric(mean_age_day) / 30.44)
mean_age_remaining_day <- round((as.numeric(mean_age_day) - mean_age_months * 30.44),0)
min_age_months <- round((as.numeric(min_age_day) / 30.44), 1)
max_age_months <- round((as.numeric(max_age_day) / 30.44), 1)


sample_size <- ma_data %>%
  summarize(mean_ss = mean(n_1),
            sd_ss = sd(n_1))
```

Our final sample of  `r n_effect_sizes` conditions reflected  `r unique_infants` unique infants (mean age: `r mean_age_months` months; `r mean_age_remaining_day` days; $SD$ = `r SD_age_day`; age range: `r min_age_months` to `r max_age_months` months), with a mean sample size of `r sample_size$mean_ss` ($SD$ = `r sample_size$sd_ss`) children per condition. Figure\ \@ref(fig:forest) shows effect size estimates for all conditions. The weighted mean effect size was `r filter(mod_print, moderator == "NULL")$estimate_print`, which significantly differed from 0  (*Z* = `r filter(mod_print, moderator == "NULL")$z_print`; *p* `r filter(mod_print,  moderator == "NULL")$p_print`). There was evidence for considerable heterogeneity in effect sizes across our sample (*Q* = `r filter(mod_print, moderator == "NULL")$Q_print`; *p* `r filter(mod_print, moderator == "NULL")$Qp_print`), meaning that there is unexplained variance in effect sizes across studies.

## Evidential value of the syntactic bootstrapping literature

We first evaluated the evidential value of the literature by assessing the evidence for publication bias. The intuition underlying these analyses is that, due to random variation, a literature should be expected to contain studies both with and without statistically significant effect sizes for the target phenomenon. Critically, however, publication pressures may lead researchers to be more likely to publish findings with statistically significant results, resulting in a biased literature. The absence of these studies from the meta-analysis yields a meta-analytic estimate that over-estimates the true effect size, and threatens the evidential value of the literature. We present two analyses that assess publication bias in the syntactic bootstrapping literature: a classic funnel plot analysis, and a sensitivity analysis that assumes a more plausible model of the publication process.

```{r label = "funnel", fig.pos = "t", fig.height = 3.75, fig.width = 4, fig.cap = 'Funnel plot showing the standard error of each effect size estimate in our meta-analysis as a function of effect size. The gray and red vertical dashed lines correspond to an effect size of zero and the meta-analytic effect size estimate, respectively. The grey funnel represents a 95% confidence interval around the meta-analytic estimate. In the absence of publication bias, effect size estimates should be symmetrically distributed around the red line. The point "bands" are due to the fact that researchers tend to use similar sample sizes across studies (e.g. many studies have 8 or 12 participants per condition).'}
generate_funnel_plot(ma_data)

reg_result <- rma.mv(d_calc ~ sqrt(d_var_calc),  d_var_calc,  
                         random = ~ 1 | short_cite/same_infant/row_id, data=ma_data) # access zval and pval from here
reg_z <- round(reg_result$zval[2], digits = 2)
p_val <- reg_result$pval[2] 
reg_p <- case_when(
 p_val < 0.0001 ~ "< .0001", 
 TRUE ~ as.character(p_val))
#reg_result
```

Figure\ \@ref(fig:funnel) presents the funnel plot for the effect sizes in our sample. A funnel plot shows estimates of effect size variance (plotted with larger values lower on the axis) as a function of the magnitude of the effect size [@egger1997bias]. Under a model of publication bias in which researchers decide whether or not to publish a study based on the magnitude of its effect size (larger effect sizes being more likely), effect size estimates should fall symmetrically around the grand effect size estimate. Evidence of asymmetry around the grand mean, particularly more large, positive effect sizes, would suggest that the literature reflects a biased sample of studies. A formal test of asymmetry in our sample revealed evidence for asymmetry (Egger's test: *Z* = `r reg_z`; *p* `r reg_p`).

The funnel plot analysis provides some evidence for publication bias, but the interpretation of this analysis is limited by the fact that it assumes a relatively implausible model of how researchers decide which studies to make public: the criteria for publishing a study in a journal is typically not the *size* of the effect, as assumed by the funnel plot analysis, but rather whether or not the p-value of the hypothesis test for that effect is below some threshold (usually .05). We therefore conducted a second analysis of publication bias, called a sensitivity analysis [@mathur2020sensitivity], which assumes that the decision to publish results is determined by the size of the p-value, rather than the magnitude of the effect size.


The goal of the sensitivity analysis is to determine how sensitive the meta-analytic effect size is to "missing" non-significant studies. Critically, because the degree of publication bias is not known (i.e., the degree to which significant results are more likely to be published, relative to insignificant results), the sensitivity analysis assumes a worst-case publication bias scenario and estimates the meta-analytic effect size under this scenario. The worst-case scenario assumed by the model is that significant studies are infinitely more likely to be published than non-significant studies ^[Technically, the model assumes studies with effect sizes that are statistically significant (*p* < .05) *and* greater than zero are infinitely more likely to be published. See Mathur and VanderWeele (2020) for additional details.]. A meta-analytic effect size under this scenario can be estimated by analyzing only those studies with non-significant effect size estimates.

```{r}
ma_data_with_affirm <- ma_data %>%
  mutate(pvalue =  2 * (1 - pnorm( abs(d_calc / sqrt(d_var_calc)))),
         affirm =  (d_calc > 0) & (pvalue < 0.05))

affirm_model<- rma.mv(d_calc,  d_var_calc,  
                         random = ~ 1 | short_cite/same_infant/x_1, data=
          ma_data_with_affirm %>% filter(affirm == FALSE)) 

worst_case_estimate_print <- paste0(as.numeric(round(affirm_model$beta, 2)),
                                    " [",
                                    as.numeric(round(affirm_model$ci.lb, 2)),
                                    ", ",                                                                                                        as.numeric(round(affirm_model$ci.ub, 2)),
                                    "]"
)
```

Conducting this sensitivity analysis on our data reveals that no amount of publication bias could attenuate the point estimate of the effect size to 0. Nevertheless, the worst-case scenario appreciably attenuates the meta-analytic effect size, and the attenuated effect size estimate includes 0 in its 95% confidence interval (`r worst_case_estimate_print`; see SI Sec. 5 for additional details). 

In sum, across two types of analyses, we find some evidence for publication bias in the syntactic bootstrapping literature, but even under worst-case scenarios publication bias was not enough to fully attenuate the meta-analytic point estimate to 0. Further, some of the publication bias observed in the funnel plot analysis may be due to heterogeneity in the data. In the following sections, we analyze theoretical and methodological moderators that may contribute to this heterogeneity, though we emphasize that the likely presence of publication bias implies that these moderators should be interpreted with caution. 

## Theoretical Moderators

We next asked whether the overall effect size estimate was moderated by our theoretical moderators of interest: development-related moderators (vocabulary and age), and sentence structure moderators (predicate and noun phrase types).


```{r, label = "development", fig.pos = "t",  fig.height = 4.8, fig.width = 4.8, fig.cap = "Syntactic bootstrapping effect size (Cohen's *d*) as a function of age in months. Each point corresponds to one effect size (condition), and point size corresponds to the number of children  in that condition. The blue line shows a linear model fit and the corresponding standard error. The dashed line indicates an effect size of zero. The slope of the model fit does not significantly differ from zero, suggesting no appreciable developmental change in the size of the syntactic bootstrapping effect.  "}

ma_data %>%
  ggplot() +
  geom_point(aes(x = mean_age, y = d_calc, size = n_1),  alpha = .7) +
  geom_smooth(method = "lm",aes(x = mean_age, y = d_calc)) +
  scale_x_continuous(labels = function(x) round(x/30.44), 
                     breaks = seq(12,48*30.44, 6*30.44)) +
  geom_hline(yintercept = 0, linetype = "dashed")+
  geom_smooth(method = "lm",
              aes(x = mean_age, y = d_calc),
              ) +
  ggtitle("Syntactic bootstrapping effect across development") +
  xlab("Mean Age (months)") + 
  ylab(expression(paste("Effect Size (Cohen's ", italic(d), ")")))   +
  guides(colour = FALSE, size = FALSE)
```

### Development

```{r}
age_vocab_cor <- cor.test(ma_data$mean_age_months,ma_data$productive_vocab_median)
cor_p <- ifelse(round(age_vocab_cor$p.value, digits = 2)<0.0001, "<.0001", as.character(round(age_vocab_cor$p.value, digits = 2)))
```



```{r label = "megaPlot", fig.pos = "t", fig.width = 10, fig.height = 5, fig.cap = "Meta-analytic models parameter estimates for (a) theoretical and (b) methodological moderators. Blue points show model estimates from single-predictor model; grey points show model estimates from additive linear model with all moderators included. Ranges correspond to 95% confidence intervals. Levels for categorical variables are given in parentheses, with the first level indicating the base level in the model."}


theoretical_mod <- rma.mv(d_calc ~ sentence_structure + agent_argument_type + mean_age_months, V = d_var_calc,
                    random = ~ 1 | short_cite/same_infant/row_id,
                     method = "REML",
                     data = ma_data) 

theoretical_mod_print <- generate_mega_model_df(theoretical_mod)

methodological_mod <- rma.mv(d_calc ~ character_identification + practice_phase + presentation_type + test_mass_or_distributed + n_repetitions_sentence,
                             V = d_var_calc,
                    random = ~ 1 | short_cite/same_infant/row_id,
                     method = "REML",
                     data = ma_data)
methodological_mod_print <- generate_mega_model_df(methodological_mod)


p_theoretical <- generate_predictor_plot(mod_print, theoretical_mod_print, "theoretical") +
  theme(legend.position = "none") + 
  ggtitle("Theoretical Moderators") +
  plot_annotation(tag_levels = 'a')

p_model <- generate_predictor_plot(mod_print, methodological_mod_print, "methodological") + 
  ggtitle("Methodological Moderators") +
  plot_annotation(tag_levels = 'a')

p_model_pane <- p_theoretical + p_model

p_model_pane  
```

How does the strength of the syntactic bootstrapping effect change across development? We examined two measures of developmental change: age (months) and vocabulary size. These two measures were strongly correlated with each other (*r*(`r age_vocab_cor$parameter`) = `r round(age_vocab_cor$estimate, digits = 2)`, *p* `r cor_p`). There was no effect of either measure on the strength of the syntactic bootstrapping effect (age: $\beta$ = `r filter(mod_print, moderator == "mean_age_months")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "mean_age_months")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "mean_age_months")$mod_z_print`, *p* `r filter(mod_print, moderator == "mean_age_months")$mod_p_print`;  Fig. \@ref(fig:development); vocabulary size: $\beta$ = `r filter(mod_print, moderator == "productive_vocab_median")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "productive_vocab_median")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "productive_vocab_median")$mod_z_print`, *p* `r filter(mod_print, moderator == "productive_vocab_median")$mod_p_print`).

### Sentence structure 
```{r summarize_sentence_structure}
ss_summary <- ma_data %>% 
  group_by(sentence_structure) %>% 
  summarise(
    mean = mean(d_calc), 
    sd = sd(d_calc)
  )

agent_argument_summary <- ma_data %>% 
  group_by(agent_argument_type) %>% 
  summarise(
    mean = mean(d_calc), 
    sd = sd(d_calc)
  )
```

We next asked how properties of the sentence structure influenced the strength of the syntactic bootstrapping effect. Predicate type (transitive vs. intransitive) was a significant moderator, ($\beta$ = `r filter(mod_print, moderator == "sentence_structure")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "sentence_structure")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "sentence_structure")$mod_z_print`, *p* `r filter(mod_print, moderator == "sentence_structure")$mod_p_print`): the effect was larger for transitive sentences, relative to intransitive sentences. Further, the model intercept did not significantly differ from zero ($\beta$ = `r filter(mod_print, moderator == "sentence_structure")$estimate_print_full`, *z* = `r filter(mod_print, moderator == "sentence_structure")$z_print`, *p* `r filter(mod_print, moderator == "sentence_structure")$p_print`), which suggests that the effect is only present in transitive conditions (*M* = `r filter(ss_summary, sentence_structure == "transitive")$mean`, *SD* = `r filter(ss_summary, sentence_structure == "transitive")$sd`) but not in intransitive conditions (*M* = `r filter(ss_summary, sentence_structure == "intransitive")$mean`, *SD* = `r filter(ss_summary, sentence_structure == "intransitive")$sd`). In contrast, there was no effect of agent argument type (pronoun: *M* = `r filter(agent_argument_summary, agent_argument_type == "pronoun")$mean`, *SD* = `r filter(agent_argument_summary, agent_argument_type == "pronoun")$sd`; noun: *M* = `r filter(agent_argument_summary, agent_argument_type == "noun")$mean`, *SD* = `r filter(agent_argument_summary, agent_argument_type == "noun")$sd`; $\beta$ = `r filter(mod_print, moderator == "agent_argument_type")$mod_estimate_print_full`, *SE* = `r filter(mod_print, moderator == "agent_argument_type")$mod_SE_print`, *z* = `r filter(mod_print, moderator == "agent_argument_type")$mod_z_print`, *p* `r filter(mod_print, moderator == "agent_argument_type")$mod_p_print`). 

To compare the effects of all theoretical moderators, we fit an additive model with all theoretical variables as fixed effects. We excluded vocabulary size because it was highly correlated with age, and was only available for a subset of conditions (*N* = `r  num_vocabulary_available`). Figure \@ref(fig:megaPlot)a shows estimates for each of the single-predictor models along with the additive linear model. The additive model revealed estimates that were highly comparable to the single-predictor model.  

In summary, we found that predicate type is a significant predictor of the effect size: conditions with transitive sentences were associated with larger effect sizes than those tested with intransitive sentences. No other theoretical variable significantly moderated the syntactic bootstrapping effect.

## Methodological Moderators 

One limiting factor in interpreting the moderating  role of theoretical variables is that there was appreciable variability across studies in the exact method used in testing children. It is possible that this methodological variability conceals true underlying moderating influences. For example, if researchers adapt their method to the age of the children they are targeting,  developmental change in the strength of the effect may not be detectable [@bergmann2018promoting]. 

```{r}
testing_procedure_summary <- ma_data %>% 
  group_by(test_mass_or_distributed) %>% 
  summarise(
    mean = mean(d_calc), 
    sd = sd(d_calc)
  )
```

```{r message=FALSE, warning=FALSE, echo=FALSE,results='hide'}
MAPPING_STUDIES <- c("Mutual exclusivity", 
                     "Sound symbolism",
                     "Cross-situational word learning",
                     "Gaze following")

# version cached Nov29
METALAB_PATH <- here("data/metalab/metalab_raw.csv")
#download_metalab_data(METALAB_PATH)

metalab_data <- read_csv(METALAB_PATH)


all_models_res_younger <- map_df(MAPPING_STUDIES, 
                                 get_model_results_younger_than, 
                                 48, 
                                 metalab_data)
SB_res_younger <- fit_model(filter(ma_data, mean_age < 30.44 * 48), "Syntactic Bootstrapping")
all_models_res_younger <- bind_rows(all_models_res_younger, SB_res_younger) 

tidy_all_models_res_younger <-  tidy_metalab_df(all_models_res_younger)
tidy_all_age_summary <- summarize_metalab_age_younger_than(ma_data, 
                                                           48, 
                                                           metalab_data)
tidy_models_with_age_younger <- left_join(tidy_all_models_res_younger, 
                                  tidy_all_age_summary, 
                                  by = "dataset_name") 
ML_print <- get_metalab_es_print(tidy_all_models_res_younger)

```

```{r}
metalab_num_condition <- metalab_data %>% 
  group_by(dataset) %>% 
  count()
```


To evaluate this possibility, we asked whether five different methodological variables (practice phase, sentence repetitions, character identification phase, synchronicity, testing procedure structure) moderated the syntactic bootstrapping effect. None of these methodological variables were significant moderators of the effect in a single predictor model (Fig. \@ref(fig:megaPlot)b; see SI, Sec. 6). In an additive linear model with all five methodological predictors, there was a significant effect of testing procedure structure ($\beta$ = `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_estimate_print_full`, *SE* = `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_SE_print`, *z* = `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_z_print`, *p* `r filter(methodological_mod_print, moderator_name == "test_mass_or_distributedmass")$mod_p_print`), with mass testing designs (*M* = `r filter(testing_procedure_summary, test_mass_or_distributed == "mass")$mean`, *SD* = `r filter(testing_procedure_summary, test_mass_or_distributed == "mass")$sd`) tending to have larger effect sizes than distributed designs (*M* = `r filter(testing_procedure_summary, test_mass_or_distributed == "distributed")$mean`, *SD* = `r filter(testing_procedure_summary, test_mass_or_distributed == "distributed")$sd`). This finding suggests that children tested in a procedure with only one train-test pair performed better than those tested in a procedure with multiple train-test pairs. Finally, we asked how these methodological moderators related to our theoretical moderators of interest. Notably, controlling for methodological variables did not qualitatively change the role of any of the theoretical moderators (see SI, Sec. 7).  Taken together, these analyses suggest that methodological variables do not play a large influencing role on the size of the syntactic bootstrapping effect. 




## Relating the syntactic bootstrapping effect to other word learning strategies

```{r echo=FALSE, results='hide', label = "metalab", fig.pos="!b", fig.height = 5, fig.width = 7, fig.cap = "Meta-analytic effect sizes of five word learning phenomena, including syntactic bootstrapping (red). Point size corresponds to  the number of individual conditions included in each meta-analysis. The x-axis shows the magnitude of the meta-analytic effect size estimate; the y-axis shows the mean age in months of children in each meta-analysis."}
#generate_metalab_plot(tidy_models_with_age_younger)

label_position <- tibble(
  "label" = c("Mutual Exclusivity", 
              "Sound Symbolism",
              "Cross-Situational Word Learning",
              "Gaze Following", 
               "Syntactic Bootstrapping"
              ), 
  color = c(
    "black", 
    "black", 
    "black", 
    "black", 
    "red"
  ),
  label_x = c(tidy_models_with_age_younger$mean_age_in_month[1] + 2, 
              tidy_models_with_age_younger$mean_age_in_month[2:4]+1.8,
            tidy_models_with_age_younger$mean_age_in_month[5]-1.8), 
  label_y = tidy_models_with_age_younger$estimate
)

tidy_models_with_age_younger %>% 
    mutate(
      text_color = case_when(
        dataset_name == "Syntactic Bootstrapping" ~ "red", 
        TRUE ~  "black"
      ), 
      dataset_name = case_when(
        dataset_name == "Cross-situational word learning" ~ "Cross-Situational Word Learning",
      #  dataset_name == "Syntactic Bootstrapping" ~ "Syntactic \n Bootstrapping",
        dataset_name == "Mutual exclusivity" ~ "Mutual Exclusivity",
        dataset_name == "Gaze following" ~ "Gaze Following", 
        dataset_name == "Sound symbolism" ~ "Sound Symbolism", 
        TRUE ~ dataset_name
      )
    ) %>% 
    ggplot(aes(x = mean_age_in_month,
               y = estimate,
               color = text_color
    )) + 
   # guides(size = FALSE) +
    xlim(0,36) +
    coord_flip() +
    geom_point(aes(size = num_study)) +
    scale_color_manual(values = c("black", "red")) + 
    guides(color = FALSE) +
    geom_text(data = label_position, 
              aes(label = label, 
                  x = label_x, 
                  y = label_y, 
                  color = color, 
                 ), size = 2)+
    geom_linerange(aes(ymin = estimate.cil, 
                       ymax = estimate.cih),size = 0.5) +
    geom_hline(yintercept = 0, color = "black", linetype="dashed")+
    scale_size_continuous(name = expression(paste(italic(N), " conditions"))) +
    ggtitle("Effect size estimates of word learning phenomena") +
    xlab("Mean Age (months)") + 
    ylab(expression(paste("Effect Size (Cohen's ", italic(d), ")")))



```

How does the strength of the syntactic bootstrapping effect compare to that of other word learning strategies? To answer this question, we compared the meta-analytic syntactic bootstrapping effect size to effect sizes for other word learning strategies estimated from a meta-analysis of each literature. We considered an opportunity sample of word learning strategies, based on those strategies with available meta-analytic data. In particular, we selected all word learning strategies available in a database of language acquisition meta-analyses, called Metalab [@bergmann2018promoting]. We included a word learning strategy in our analysis if it could be considered to facilitate an inference about the mapping between a novel word and a meaning.  This allowed for the comparison of the syntactic bootstrapping effect to four additional word learning strategies: (i) mutual exclusivity, assuming a novel word refers to a novel object [@lewis2020role; @markman1988children; @clark1987principle], (ii) cross-situational word learning, tracking word-object co-occurrences across situations [@yu2007rapid], (iii) gaze following, following the eye gaze of a speaker to the intended referent [@frank2016performance; @scaife1975capacity], and (iv) sound symbolism, exploiting sound-meaning regularities in the lexicon [@fort2018symbouki]. While these four strategies are not exhaustive of the strategies that have been proposed in the word learning literature, they are representative of the major theoretical perspectives, including constraints and biases [@markman1990constraints], statistical learning [@romberg2010statistical], and communicative inferences [@tomasello2010origins]. For each of these four comparison strategies, we calculated the meta-analytic effect size using the same model specification as for syntactic bootstrapping, restricting the sample to studies with a mean age of children younger than  48-month-olds.


Figure \@ref(fig:metalab) shows the meta-analytic effect size for syntactic bootstrapping and each of the other four word learning strategies.  The syntactic bootstrapping effect size (`r filter(mod_print, moderator == "NULL")$estimate_print`; *N* conditions = `r n_effect_sizes`; *M* age = `r round(as.numeric(mean_age_day)/30.44, 1)` mo) was comparable in size to that of sound symbolism (*d* = `r filter(ML_print, dataset_name == "Sound symbolism")$estimate_print_full`; *N* conditions = `r filter(metalab_num_condition, dataset == "Sound symbolism")$n`; *M* age = `r round(filter(tidy_all_age_summary, dataset == "Sound symbolism")$mean_age_in_month, 1)` mo) and cross-situational learning (*d* = `r filter(ML_print, dataset_name == "Cross-situational word learning")$estimate_print_full`; *N* conditions = `r filter(metalab_num_condition, dataset == "Cross-situational word learning")$n`; *M* age = `r round(filter(tidy_all_age_summary, dataset == "Cross-situational word learning")$mean_age_in_month, 1)` mo), and less than a quarter of the size of both mutual exclusivity (*d* = `r filter(ML_print, dataset_name == "Mutual exclusivity")$estimate_print_full`; *N* conditions = `r filter(metalab_num_condition, dataset == "Mutual exclusivity")$n`; *M* age = `r round(filter(tidy_all_age_summary, dataset == "Mutual exclusivity")$mean_age_in_month, 1)` mo) and gaze following (*d* = `r filter(ML_print, dataset_name == "Gaze following")$estimate_print_full`; *N* conditions = `r filter(metalab_num_condition, dataset == "Gaze following")$n`; *M* age = `r round(filter(tidy_all_age_summary, dataset == "Gaze following")$mean_age_in_month, 1)` mo). Importantly, the small effect size of syntactic bootstrapping relative to mutual exclusivity and gaze following cannot  be due alone to differences in the ages of the samples in these different meta-analyses, because participants in the syntactic bootstrapping meta-analysis were older on average than those in the gaze following meta-analysis, and roughly the same age as those in the mutual exclusivity meta-analysis. 

